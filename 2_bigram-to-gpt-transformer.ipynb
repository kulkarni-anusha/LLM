{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      },
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "Due Date Feb 4th, 2024 11:59pm\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK7k8dpLG3Y7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qra06Ema_VL"
      },
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iradmn7bZtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30562d82-e9d6-47a8-b840-cce878752c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-05 19:15:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-05 19:15:26 (28.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLoVi294G-T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97076250-d29d-4884-c937-163f60f3b8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "# part 1a\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text\n",
        "chars = sorted(set(text))\n",
        "\n",
        "# part 1b\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [chars.index(c) for c in s]\n",
        "\n",
        "# part 1c\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join(chars[i] for i in ids)\n",
        "\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoMGZgEOdRjt"
      },
      "outputs": [],
      "source": [
        "# part 1d\n",
        "def create_one_hot_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    input_seqs = [text[i:i+2] for i in range(len(text)-1)]\n",
        "    inputs_one_hot = torch.zeros(len(input_seqs), len(chars))\n",
        "    outputs_one_hot = torch.zeros(len(input_seqs), len(chars))\n",
        "\n",
        "    for i, seq in enumerate(input_seqs):\n",
        "        inputs_one_hot[i, encode(seq[0])] = 1\n",
        "        outputs_one_hot[i, encode(seq[1])] = 1\n",
        "\n",
        "    return inputs_one_hot, outputs_one_hot\n",
        "\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PasrfDz-dSqx",
        "outputId": "ccd52536-9a26-4971-86f0-3b1c0ed550e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated word: aYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n"
          ]
        }
      ],
      "source": [
        "# part 1e\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BigramOneHotMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(len(chars), 8)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        generated_text = start\n",
        "        current_char = start\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_tensor = torch.zeros(1, len(chars))\n",
        "            input_tensor[0, encode(current_char)] = 1\n",
        "            output_tensor = self.forward(input_tensor)\n",
        "            _, next_char_idx = torch.max(output_tensor, 1)\n",
        "            next_char = decode(next_char_idx.tolist())\n",
        "            generated_text += next_char\n",
        "            current_char = next_char\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP()\n",
        "\n",
        "generated_word = bigram_one_hot_mlp.generate()\n",
        "print(f'Generated word: {generated_word}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odOs6PjdyJUW",
        "outputId": "0a3595b2-6c37-47bc-8740-526ad72809d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a                                                                                                    \n"
          ]
        }
      ],
      "source": [
        "# part 1f\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(bigram_one_hot_mlp.parameters(), lr=0.01)\n",
        "\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = bigram_one_hot_mlp(inputs_one_hot)\n",
        "    loss = criterion(outputs, torch.argmax(outputs_one_hot, dim=1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(bigram_one_hot_mlp.generate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRJQcl-AyJk9"
      },
      "outputs": [],
      "source": [
        "# part 1g\n",
        "def create_embedding_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    input_seqs = [text[i:i+2] for i in range(len(text)-1)]\n",
        "    input_ids = torch.tensor([encode(seq[0]) for seq in input_seqs])\n",
        "    outputs_one_hot = torch.zeros(len(input_seqs), len(chars))\n",
        "\n",
        "    for i, seq in enumerate(input_seqs):\n",
        "        outputs_one_hot[i, encode(seq[1])] = 1\n",
        "\n",
        "    return input_ids, outputs_one_hot\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnmLqBBAyZr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d125847-3a95-41ab-97e4-e08384b4bbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a                                                                                                    \n",
            "an                                                                                                   \n"
          ]
        }
      ],
      "source": [
        "# part 1h\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BigramEmbeddingMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(chars), 8)\n",
        "        self.fc1 = nn.Linear(8, 8)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.sum(x, dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        generated_text = start\n",
        "        current_char = start\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_tensor = torch.tensor([encode(current_char)])\n",
        "            output_tensor = self.forward(input_tensor)\n",
        "            _, next_char_idx = torch.max(output_tensor, 1)\n",
        "            next_char = decode(next_char_idx.tolist())\n",
        "            generated_text += next_char\n",
        "            current_char = next_char\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "optimizer_embed = optim.SGD(bigram_embedding_mlp.parameters(), lr=0.01)\n",
        "\n",
        "for _ in range(1000):\n",
        "    optimizer_embed.zero_grad()\n",
        "    outputs_embed = bigram_embedding_mlp(input_ids)\n",
        "    loss_embed = criterion(outputs_embed, torch.argmax(outputs_one_hot, dim=1))\n",
        "    loss_embed.backward()\n",
        "    optimizer_embed.step()\n",
        "\n",
        "# Generating text using the trained models\n",
        "print(bigram_one_hot_mlp.generate())\n",
        "print(bigram_embedding_mlp.generate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplpM8_Cbp0s"
      },
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oh-3FeFxxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce8c265-b63b-4857-a4f5-73c5961eac0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "outputs": [],
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      },
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnEOfMj4Dk4Y"
      },
      "outputs": [],
      "source": [
        "# Implement character-level tokenization function\n",
        "def tokenize(text):\n",
        "    return sorted(set(text))\n",
        "\n",
        "# part 2.1\n",
        "chars = tokenize(text)\n",
        "\n",
        "# part 2.2\n",
        "\n",
        "# Implement encode function\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [chars.index(c) for c in s]\n",
        "\n",
        "# Implement decode function\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join(chars[i] for i in ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gyOaRF5Dq1P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "2925c670-2cfb-4a9d-a76e-6fdf4e3d33a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-43494d17d929>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvWGi8Mk6x1q"
      },
      "outputs": [],
      "source": [
        "block_size = 16\n",
        "data[:block_size+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvO4hSK171Vu"
      },
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVWxO6Pa70Lh"
      },
      "outputs": [],
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFYZnm2MuLlt"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HmnXJjxtm3N"
      },
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SD8Z16R-sfZ"
      },
      "outputs": [],
      "source": [
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(64, head_size, bias=False).to(device)\n",
        "        self.k_proj = nn.Linear(64, head_size, bias=False).to(device)\n",
        "        self.v_proj = nn.Linear(64, head_size, bias=False).to(device)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        B, T, C = x.shape\n",
        "        k = self.k_proj(x)      # (B,T,C)\n",
        "        q = self.q_proj(x)      # (B,T,C)\n",
        "\n",
        "        attention = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        mask = torch.tril(torch.ones(T, T)).to(torch.bool).to(device)\n",
        "        masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "        masked_attention = F.softmax(masked_attention, dim=-1) # (B, T, T)\n",
        "        masked_attention = self.dropout(masked_attention)\n",
        "        v = self.v_proj(x).to(device)\n",
        "        out = masked_attention @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "x = torch.randn((8,32, 64)).to(device)\n",
        "attn = SelfAttentionHead(16).to(device)\n",
        "print(attn(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWeoHGBiFpWd"
      },
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFsPDkpnFs_b"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Create attention heads using nn.ModuleList\n",
        "        self.attention_heads = nn.ModuleList([\n",
        "            SelfAttentionHead(head_size) for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        # Linear layer for output projection\n",
        "        self.output_projection = nn.Linear(num_heads * head_size, head_size * num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Iterate through heads, applying attention and concatenating outputs\n",
        "        head_outputs = [head(x) for head in self.attention_heads]\n",
        "        concatenated_output = torch.cat(head_outputs, dim=2)  # Concatenate along feature dimension\n",
        "\n",
        "        # Pass concatenated output through linear layer\n",
        "        output = self.output_projection(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "x = torch.randn((8,32, 64)).to(device)\n",
        "attn = MultiHeadAttention(num_heads = 4, head_size=16).to(device)\n",
        "print(attn(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      },
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K96Z3kAv7lNt"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(64, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(256, 64)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "model = MLP()\n",
        "input_tensor = torch.randn(8, 32, 64)\n",
        "output_tensor = model(input_tensor)\n",
        "#print(output_tensor)  # Print the entire output tensor\n",
        "print(output_tensor.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUFxuyf-JIxr"
      },
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTDAd66KIvvx"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(n_embd)\n",
        "        self.heads=MultiHeadAttention(n_head,16)\n",
        "        self.fc1 = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y=self.norm1(x)\n",
        "        y=self.heads(y)\n",
        "        x=x+y\n",
        "        z=self.norm1(x)\n",
        "        z=self.fc1(z)\n",
        "        x=x+z\n",
        "        return x\n",
        "\n",
        "model = Block(n_embd=64, n_head=4).to(device)\n",
        "input_tensor = torch.randn(8, 32, 64).to(device)\n",
        "output_tensor = model(input_tensor)\n",
        "print(output_tensor.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFQXltDKNti"
      },
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      },
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbm0vaS7G0bh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "%env CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WT4oUN084ts"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "import numpy as np\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        # create the token embedding table\n",
        "        self.token_embedding = nn.Embedding(len(chars),n_embd)\n",
        "\n",
        "        # Create the position embedding table\n",
        "        self.positional_embedding = nn.Embedding(32, n_embd)\n",
        "        # Create dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Create blocks using nn.Sequential to go through series of 4 blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(4)])\n",
        "\n",
        "        # Create a layer norm layer\n",
        "        self.layer_norm = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Create a linear layer for predicting the next token\n",
        "        self.linear = nn.Linear(n_embd, len(chars))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "            idx = idx.view(-1, idx.size(-1))\n",
        "            B,T=idx.shape\n",
        "            idx = idx.to(self.token_embedding.weight.device)\n",
        "            token_embeddings = self.token_embedding(idx).to(device)\n",
        "            positional_embedding = self.positional_embedding(torch.arange(T, device=device))\n",
        "            x = token_embeddings + positional_embedding\n",
        "            x = self.dropout(x)\n",
        "            x= self.blocks(x)\n",
        "            x = self.layer_norm(x)\n",
        "            logits = self.linear(x)\n",
        "            if targets is None:\n",
        "                loss = None\n",
        "            else:\n",
        "                B, T, C = logits.shape\n",
        "                logits = logits.view(B*T, C)\n",
        "                targets = targets.view(B*T)\n",
        "                loss = F.cross_entropy(logits, targets)\n",
        "            return logits,loss\n",
        "\n",
        "\n",
        "    def generate(self, start_char, max_new_tokens, top_p, top_k, temperature):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            current_seq = torch.tensor([encode(start_char)], dtype=torch.long).to(device).unsqueeze(0)\n",
        "            for _ in range(max_new_tokens):\n",
        "                logits, loss = self(current_seq[:, -1:])\n",
        "                logits = logits[:, -1, :]\n",
        "                scaled_logits = logits / temperature\n",
        "                probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "                if top_k is not None:\n",
        "                    sampled_index = top_k_sampling(probabilities, k=top_k)\n",
        "                elif top_p is not None:\n",
        "                    sampled_index = top_p_sampling(probabilities, p=top_p)\n",
        "                sampled_token = torch.tensor([[sampled_index]]).unsqueeze(-1)\n",
        "                current_seq = torch.cat([current_seq, sampled_token], dim=1)\n",
        "        generated_string = decode([token.item() for token in current_seq[0]])\n",
        "        return generated_string\n",
        "\n",
        "def top_k_sampling(probabilities, k=5):\n",
        "        probabilities = probabilities.cpu().numpy().flatten()\n",
        "        top_k_indices = np.argsort(probabilities)[-k:]\n",
        "        top_k_probabilities = probabilities[top_k_indices]\n",
        "        top_k_probabilities /= top_k_probabilities.sum()\n",
        "        chosen_index = np.random.choice(top_k_indices, p=top_k_probabilities)\n",
        "\n",
        "        return chosen_index\n",
        "\n",
        "def top_p_sampling(probabilities, p=0.9):\n",
        "    probabilities = probabilities.cpu().numpy().flatten()\n",
        "    if len(probabilities) > 1:\n",
        "      sorted_indices = np.argsort(probabilities)[::-1]\n",
        "    else:\n",
        "      sorted_indices = np.argsort(probabilities)\n",
        "\n",
        "    sorted_probabilities = probabilities[sorted_indices]\n",
        "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
        "    cutoff_index = np.where(cumulative_probabilities > p)[0][0]\n",
        "    filtered_indices = sorted_indices[:cutoff_index + 1]\n",
        "\n",
        "    filtered_probabilities = sorted_probabilities[:cutoff_index + 1]\n",
        "    filtered_probabilities /= filtered_probabilities.sum()\n",
        "    chosen_index = np.random.choice(filtered_indices, p=filtered_probabilities)\n",
        "\n",
        "    return chosen_index\n",
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cpu'\n",
        "gpt_model = GPT(64, 4).to(device)\n",
        "generated_text = gpt_model.generate(start_char='a', max_new_tokens=100,top_k=None,top_p=0.9, temperature=1.0)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njzrwwiv-mfB"
      },
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWtn2uTwYUrY"
      },
      "outputs": [],
      "source": [
        "gpt_model = GPT(64, 4).to(device)\n",
        "# make you are running this on the GPU\n",
        "max_iters = 10000\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(gpt_model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "  optimizer.zero_grad()\n",
        "  xb, yb = get_batch()\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if iter%1000==0:\n",
        "      print(f'Iteration {iter+1}, Loss: {loss.item()}')\n",
        "      print(f'Generated text:{gpt_model.generate(start_char=\"a\", max_new_tokens=100,top_k=5,top_p=None, temperature=1.0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      },
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l4soWviWG5M"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "print(gpt_model.generate(start_char=\"a\", max_new_tokens=100,top_k=5,top_p=None, temperature=1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0MVJvGLhnY4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}